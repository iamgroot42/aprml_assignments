{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "sys.version_info(major=2, minor=7, micro=6, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import itertools    \n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU, ELU\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Input, Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l1,l2\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix',cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_classwise(y_test, y_pred, name):\n",
    "    y_test_aho = np.argmax(y_test, axis=1)\n",
    "    y_pred_aho = np.argmax(y_pred, axis=1)\n",
    "    print(classification_report(y_test_aho, y_pred_aho))\n",
    "    plt.figure()\n",
    "    cnf_matrix = confusion_matrix(y_test_aho, y_pred_aho)\n",
    "    plot_confusion_matrix(cnf_matrix, range(10),title='Confusion matrix')\n",
    "    plt.savefig(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ChannelIgnore(Layer):\n",
    "\n",
    "    def __init__(self, drop_ratio=2, **kwargs):\n",
    "        super(ChannelIgnore, self).__init__(**kwargs)\n",
    "        assert(drop_ratio>0)\n",
    "        self.drop_ratio = drop_ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ChannelIgnore, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        y = x[:]\n",
    "        if K.image_dim_ordering() == 'tf':\n",
    "            channels = np.random.choice(x.shape[3], x.shape[3]/self.drop_ratio, replace=False)\n",
    "            for i in range(y.shape[0]):\n",
    "                y[i,:,:,channels] = 0.0\n",
    "        else:\n",
    "            channels = np.random.choice(x.shape[1], x.shape[3]/self.drop_ratio, replace=False)\n",
    "            for i in range(y.shape[0]):\n",
    "                y[:,channels,:,:] = 0.0\n",
    "        return K.in_train_phase(y,x)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(regularizer=None):\n",
    "    input_layer = Input(shape=(96,96,3))\n",
    "    \n",
    "    model = Conv2D(64, (3, 3), activation='tanh',kernel_regularizer=regularizer)(input_layer)\n",
    "    model = Conv2D(64, (3, 3), activation='tanh',kernel_regularizer=regularizer)(model)\n",
    "    model = MaxPooling2D()(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    \n",
    "    model = Conv2D(128, (3, 3), activation='tanh',kernel_regularizer=regularizer)(model)\n",
    "    model = Conv2D(128, (3, 3), activation='tanh',kernel_regularizer=regularizer)(model)\n",
    "    model = MaxPooling2D()(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    \n",
    "    model = Conv2D(256, (3, 3), activation='tanh',kernel_regularizer=regularizer)(model)\n",
    "    model = Conv2D(256, (3, 3), activation='tanh',kernel_regularizer=regularizer)(model)\n",
    "    model = MaxPooling2D()(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    \n",
    "    model = Flatten()(model)\n",
    "    model = Dense(10,kernel_regularizer=regularizer, activation='softmax')(model)\n",
    "    \n",
    "    network = Model(input_layer, model)\n",
    "    opt = keras.optimizers.Adadelta()\n",
    "    network.compile(loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_novel_model():\n",
    "    input_layer = Input(shape=(96,96,3))\n",
    "    \n",
    "    model = ChannelIgnore()(input_layer)\n",
    "    model = Conv2D(64, (3, 3), activation='tanh',kernel_regularizer=regularizer)(model)\n",
    "    model = ChannelIgnore()(input_layer)\n",
    "    model = Conv2D(64, (3, 3), activation='tanh',kernel_regularizer=regularizer)(model)\n",
    "    model = ChannelIgnore()(input_layer)\n",
    "    model = MaxPooling2D()(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    \n",
    "    model = ChannelIgnore()(input_layer)\n",
    "    model = Conv2D(128, (3, 3), activation='tanh',kernel_regularizer=regularizer)(model)\n",
    "    model = ChannelIgnore()(input_layer)\n",
    "    model = Conv2D(128, (3, 3), activation='tanh',kernel_regularizer=regularizer)(model)\n",
    "    model = ChannelIgnore()(input_layer)\n",
    "    model = MaxPooling2D()(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    \n",
    "    model = ChannelIgnore()(input_layer)\n",
    "    model = Conv2D(256, (3, 3), activation='tanh',kernel_regularizer=regularizer)(model)\n",
    "    model = ChannelIgnore()(input_layer)\n",
    "    model = Conv2D(256, (3, 3), activation='tanh',kernel_regularizer=regularizer)(model)\n",
    "    model = ChannelIgnore()(input_layer)\n",
    "    model = MaxPooling2D()(model)\n",
    "    model = BatchNormalization()(model)\n",
    "    \n",
    "    model = ChannelIgnore()(input_layer)\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(10,kernel_regularizer=regularizer, activation='softmax')(model)\n",
    "    \n",
    "    network = Model(input_layer, model)\n",
    "    opt = keras.optimizers.Adadelta()\n",
    "    network.compile(loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partA1(x_train, y_train, x_test, y_test):\n",
    "    model = build_model()\n",
    "    model.fit(x_train, y_train, batch_size=2,epochs=100,validation_split=0.2,callbacks=[TensorBoard('./partA1')])\n",
    "    y_pred = model.predict(x_test)\n",
    "    plot_classwise(y_test, y_pred, 'partA1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partA2(x_train, y_train, x_test, y_test):\n",
    "    model = build_model()\n",
    "    model.fit(x_train, y_train, batch_size=2,epochs=100,validation_split=0.2,callbacks=[TensorBoard('./partA2',histogram_freq=100)])\n",
    "    y_pred = model.predict(x_test)\n",
    "    plot_classwise(y_test, y_pred, 'partA2.png')\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partA3(x_train, y_train, x_test, y_test):\n",
    "    #L1 regularization\n",
    "#     model = build_model('l1')\n",
    "#     model.fit(x_train, y_train, batch_size=2,epochs=100,validation_split=0.2,callbacks=[TensorBoard('./partA3a',histogram_freq=100)])\n",
    "#     y_pred = model.predict(x_test)\n",
    "#     plot_classwise(y_test, y_pred, 'partA3a.png')\n",
    "#     #L2 regularization\n",
    "#     model = build_model('l2')\n",
    "#     model.fit(x_train, y_train, batch_size=2,epochs=100,validation_split=0.2,callbacks=[TensorBoard('./partA3b',histogram_freq=100)])\n",
    "#     y_pred = model.predict(x_test)\n",
    "#     plot_classwise(y_test, y_pred, 'partA3b.png')\n",
    "    #Custom regularization\n",
    "    model = build_novel_model()\n",
    "    model.fit(x_train, y_train, batch_size=2,epochs=100,validation_split=0.2,callbacks=[TensorBoard('./partA3c',histogram_freq=100)])\n",
    "    y_pred = model.predict(x_test)\n",
    "    plot_classwise(y_test, y_pred, 'partA3c.png')\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_predictions(y_pred_noreg, y_pred_reg, y_test):\n",
    "    reg_correct = (np.argmax(y_pred_reg, axis=1) == np.argmax(y_test, axis=1))\n",
    "    reg_incorrect = (np.argmax(y_pred_reg, axis=1) != np.argmax(y_test, axis=1))\n",
    "    nonreg_incorrect = (np.argmax(y_pred_noreg, axis=1) != np.argmax(y_test, axis=1))\n",
    "    A = np.logical_and(reg_correct, nonreg_incorrect)\n",
    "    B = np.logical_and(reg_incorrect, nonreg_incorrect)\n",
    "    return np.nonzero(A)[0], np.nonzero(B)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_images(prefix, images):\n",
    "    from PIL import Image\n",
    "    for i, image in enumerate(images):\n",
    "        result = Image.fromarray((image * 255).astype('uint8'))\n",
    "        result.save(prefix + str(i+1) + '.bmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test), cross_val_indices = load_data.load()\n",
    "X_train_sub = X_train[cross_val_indices[0]]\n",
    "Y_train_sub = Y_train[cross_val_indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "800/800 [==============================] - 29s - loss: 3.0243 - acc: 0.1813 - val_loss: 3.6987 - val_acc: 0.1750\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 22s - loss: 2.9000 - acc: 0.2087 - val_loss: 3.8397 - val_acc: 0.1050\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 20s - loss: 2.7390 - acc: 0.2675 - val_loss: 2.8668 - val_acc: 0.2200\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 21s - loss: 2.3780 - acc: 0.3400 - val_loss: 3.2628 - val_acc: 0.2100\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 22s - loss: 2.2198 - acc: 0.3613 - val_loss: 3.0159 - val_acc: 0.2200\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 22s - loss: 2.1093 - acc: 0.3925 - val_loss: 2.9040 - val_acc: 0.2650\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 20s - loss: 1.8068 - acc: 0.4588 - val_loss: 2.6660 - val_acc: 0.2800\n",
      "Epoch 8/100\n",
      "176/800 [=====>........................] - ETA: 16s - loss: 1.4497 - acc: 0.5170 E"
     ]
    }
   ],
   "source": [
    "partA1(X_train, Y_train, X_test, Y_test)\n",
    "\n",
    "no_regularize = partA2(X_train_sub, Y_train_sub, X_test, Y_test)\n",
    "\n",
    "regularize = partA3(X_train_sub, Y_train_sub, X_test, Y_test)\n",
    "\n",
    "parta, partb = compare_predictions(no_regularize, regularize, Y_test)\n",
    "\n",
    "inspect_A = X_test[parta]\n",
    "inspect_B = X_test[partb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_images(\"iparta/\",inspect_A)\n",
    "save_images(\"ipartb/\",inspect_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
